---
---

@inproceedings{10.1145/3544548.3580704,
abbr={CHI},
html={https://dl.acm.org/doi/fullHtml/10.1145/3544548.3580704},
pdf={https://dl.acm.org/doi/pdf/10.1145/3544548.3580704},
video={https://www.youtube.com/watch?v=ERJLWdITa5U},
bibtex_show={true},
equal_contrib={2},
selected = {true},
author = {He, Fengming and Hu, Xiyun and Shi, Jingyu and Qian, Xun and Wang, Tianyi and Ramani, Karthik},
title = {Ubi Edge: Authoring Edge-Based Opportunistic Tangible User Interfaces in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580704},
doi = {10.1145/3544548.3580704},
abstract = {Edges are one of the most ubiquitous geometric features of physical objects. They provide accurate haptic feedback and easy-to-track features for camera systems, making them an ideal basis for Tangible User Interfaces (TUI) in Augmented Reality (AR). We introduce Ubi Edge, an AR authoring tool that allows end-users to customize edges on daily objects as TUI inputs to control varied digital functions. We develop an integrated AR-device and an integrated vision-based detection pipeline that can track 3D edges and detect the touch interaction between fingers and edges. Leveraging the spatial-awareness of AR, users can simply select an edge by sliding fingers along it and then make the edge interactive by connecting it to various digital functions. We demonstrate four use cases including multi-function controllers, smart homes, games, and TUI-based tutorials. We also evaluated and proved our system’s usability through a two-session user study, where qualitative and quantitative results are positive.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {461},
numpages = {14},
keywords = {Augmented Reality, Tangible User Interface, immersive authoring},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3526113.3545663,
abbr={UIST},
bibtex_show={true},
equal_contrib={2},
selected = {true},
author = {Qian, Xun and He, Fengming and Hu, Xiyun and Wang, Tianyi and Ramani, Karthik},
title = {ARnnotate: An Augmented Reality Interface for Collecting Custom Dataset of 3D Hand-Object Interaction Pose Estimation},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545663},
doi = {10.1145/3526113.3545663},
abstract = {Vision-based 3D pose estimation has substantial potential in hand-object interaction applications and requires user-specified datasets to achieve robust performance. We propose ARnnotate, an Augmented Reality (AR) interface enabling end-users to create custom data using a hand-tracking-capable AR device. Unlike other dataset collection strategies, ARnnotate first guides a user to manipulate a virtual bounding box and records its poses and the user’s hand joint positions as the labels. By leveraging the spatial awareness of AR, the user manipulates the corresponding physical object while following the in-situ AR animation of the bounding box and hand model, while ARnnotate captures the user’s first-person view as the images of the dataset. A 12-participant user study was conducted, and the results proved the system’s usability in terms of the spatial accuracy of the labels, the satisfactory performance of the deep neural networks trained with the data collected by ARnnotate, and the users’ subjective feedback.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {41},
numpages = {14},
keywords = {Dataset Collection, Augmented Reality, 3D Pose Estimation, Hand-Object Interaction},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3491102.3517665,
abbr={CHI},
bibtex_show={true},
equal_contrib={2},
selected = {true},
author = {Qian, Xun and He, Fengming and Hu, Xiyun and Wang, Tianyi and Ipsita, Ananya and Ramani, Karthik},
title = {ScalAR: Authoring Semantically Adaptive Augmented Reality Experiences in Virtual Reality},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517665},
doi = {10.1145/3491102.3517665},
abstract = {Augmented Reality (AR) experiences tightly associate virtual contents with environmental entities. However, the dissimilarity of different environments limits the adaptive AR content behaviors under large-scale deployment. We propose ScalAR, an integrated workflow enabling designers to author semantically adaptive AR experiences in Virtual Reality (VR). First, potential AR consumers collect local scenes with a semantic understanding technique. ScalAR then synthesizes numerous similar scenes. In VR, a designer authors the AR contents’ semantic associations and validates the design while being immersed in the provided scenes. We adopt a decision-tree-based algorithm to fit the designer’s demonstrations as a semantic adaptation model to deploy the authored AR experience in a physical scene. We further showcase two application scenarios authored by ScalAR and conduct a two-session user study where the quantitative results prove the accuracy of the AR content rendering and the qualitative results show the usability of ScalAR.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {65},
numpages = {18},
keywords = {Virtual Reality, Adaptation, Semantic Understanding, Immersive Authoring, Augmented Reality},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3472749.3474769,
abbr={UIST},
bibtex_show={true},
equal_contrib={2},
author = {Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and Cao, Yuanzhi and Ramani, Karthik},
title = {GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474769},
doi = {10.1145/3472749.3474769},
abstract = {Freehand gesture is an essential input modality for modern Augmented Reality (AR) user experiences. However, developing AR applications with customized hand interactions remains a challenge for end-users. Therefore, we propose GesturAR, an end-to-end authoring tool that supports users to create in-situ freehand AR applications through embodied demonstration and visual programming. During authoring, users can intuitively demonstrate the customized gesture inputs while referring to the spatial and temporal context. Based on the taxonomy of gestures in AR, we proposed a hand interaction model which maps the gesture inputs to the reactions of the AR contents. Thus, users can author comprehensive freehand applications using trigger-action visual programming and instantly experience the results in AR. Further, we demonstrate multiple application scenarios enabled by GesturAR, such as interactive virtual objects, robots, and avatars, room-level interactive AR spaces, embodied AR presentations, etc. Finally, we evaluate the performance and usability of GesturAR through a user study.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {552–567},
numpages = {16},
keywords = {immersive authoring, Augmented Reality, embodied demonstration, Freehand interactions},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3379337.3415815,
abbr={UIST},
  bibtex_show={true},
  equal_contrib={2},
  author = {Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and Huo, Ke and Cao, Yuanzhi and Ramani, Karthik},
  title = {CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications},
  year = {2020},
  isbn = {9781450375146},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3379337.3415815},
  doi = {10.1145/3379337.3415815},
  abstract = {Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.},
  booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
  pages = {328–341},
  numpages = {14},
  keywords = {in-situ authoring, end-user programming tool, augmented reality, context-aware application, embodied authoring, ubiquitous computing},
  location = {Virtual Event, USA},
  series = {UIST '20}
}